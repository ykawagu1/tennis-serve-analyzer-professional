"""
ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ç”»åƒç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹
ãƒ†ãƒ‹ã‚¹ã‚µãƒ¼ãƒ–è§£æã‚·ã‚¹ãƒ†ãƒ ç”¨
"""
import os
import cv2
import numpy as np
from pathlib import Path
import logging
from typing import List, Dict, Any, Optional, Tuple

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OverlayImageGenerator:
    """ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ç”»åƒç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.logger = logging.getLogger(__name__)
        
    def generate_overlay_images(self, video_path: str, pose_results: List[Dict], output_dir: str) -> Dict[str, Any]:
        """
        å‹•ç”»ã‹ã‚‰ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ç”»åƒã‚’ç”Ÿæˆ
        
        Args:
            video_path: å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            pose_results: ãƒãƒ¼ã‚ºæ¤œå‡ºçµæœ
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            
        Returns:
            ç”Ÿæˆçµæœã®è¾æ›¸
        """
        try:
            self.logger.info("ğŸ¨ ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ç”»åƒç”Ÿæˆã‚’é–‹å§‹")
            
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            overlay_dir = os.path.join(output_dir, 'overlay_images')
            os.makedirs(overlay_dir, exist_ok=True)
            
            # åˆ©ãæ‰‹åˆ¤å®š
            dominant_hand = self._detect_dominant_hand(pose_results)
            self.logger.info(f"ğŸŸ¢ åˆ©ãæ‰‹åˆ¤å®š: {dominant_hand}")
            
            # ãƒˆãƒ­ãƒ•ã‚£ãƒ¼ãƒãƒ¼ã‚ºæ¤œå‡º
            trophy_frame = self._detect_trophy_pose(pose_results, dominant_hand)
            
            # ä»£è¡¨ãƒ•ãƒ¬ãƒ¼ãƒ é¸æŠ
            frame_indices = self._select_representative_frames(pose_results, trophy_frame)
            self.logger.info(f"ğŸ¯ é¸æŠãƒ•ãƒ¬ãƒ¼ãƒ : {frame_indices}")
            
            # ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ç”»åƒç”Ÿæˆ
            saved_images = self._generate_images(video_path, pose_results, frame_indices, overlay_dir)
            
            result = {
                'success': True,
                'dominant_hand': dominant_hand,
                'trophy_frame': trophy_frame,
                'frame_indices': frame_indices,
                'saved_images': saved_images,
                'image_count': len(saved_images),
                'output_directory': overlay_dir
            }
            
            self.logger.info(f"ğŸ‰ ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ç”»åƒç”Ÿæˆå®Œäº†: {len(saved_images)}æš")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ç”»åƒç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'error': str(e),
                'saved_images': [],
                'image_count': 0
            }
    
    def _detect_dominant_hand(self, pose_results: List[Dict]) -> str:
        """åˆ©ãæ‰‹ã‚’è‡ªå‹•åˆ¤å®š"""
        right_hand_raised = 0
        left_hand_raised = 0
        
        for result in pose_results:
            if result.get("has_pose") and result.get("landmarks"):
                landmarks = result["landmarks"]
                
                # å³æ‰‹ã®åˆ¤å®š
                rw = landmarks.get("right_wrist", {})
                rs = landmarks.get("right_shoulder", {})
                if rw and rs and rw.get("y", float('inf')) < rs.get("y", 0):
                    right_hand_raised += 1
                
                # å·¦æ‰‹ã®åˆ¤å®š
                lw = landmarks.get("left_wrist", {})
                ls = landmarks.get("left_shoulder", {})
                if lw and ls and lw.get("y", float('inf')) < ls.get("y", 0):
                    left_hand_raised += 1
        
        return "right" if right_hand_raised >= left_hand_raised else "left"
    
    def _detect_trophy_pose(self, pose_results: List[Dict], dominant_hand: str) -> Optional[int]:
        """ãƒˆãƒ­ãƒ•ã‚£ãƒ¼ãƒãƒ¼ã‚ºã‚’æ¤œå‡º"""
        candidate_frames = []
        
        for result in pose_results:
            if result.get("has_pose") and result.get("landmarks"):
                landmarks = result["landmarks"]
                frame_number = result.get("frame_number", 0)
                
                # åˆ©ãæ‰‹ã®ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯å–å¾—
                wrist = landmarks.get(f"{dominant_hand}_wrist", {})
                elbow = landmarks.get(f"{dominant_hand}_elbow", {})
                shoulder = landmarks.get(f"{dominant_hand}_shoulder", {})
                
                if all([wrist, elbow, shoulder]):
                    # ãƒˆãƒ­ãƒ•ã‚£ãƒ¼ãƒãƒ¼ã‚ºã®ç‰¹å¾´: è‚˜ãƒ»æ‰‹é¦–ãŒè‚©ã‚ˆã‚Šé«˜ã„
                    if (elbow.get("y", float('inf')) < shoulder.get("y", 0) and 
                        wrist.get("y", float('inf')) < shoulder.get("y", 0)):
                        candidate_frames.append((frame_number, wrist.get("y", 0)))
        
        if candidate_frames:
            # æ‰‹é¦–ãŒæœ€ã‚‚é«˜ã„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒˆãƒ­ãƒ•ã‚£ãƒ¼ãƒãƒ¼ã‚ºã¨ã™ã‚‹
            candidate_frames.sort(key=lambda x: x[1])
            trophy_frame = candidate_frames[0][0]
            self.logger.info(f"ğŸ† ãƒˆãƒ­ãƒ•ã‚£ãƒ¼ãƒãƒ¼ã‚ºæ¤œå‡º: ãƒ•ãƒ¬ãƒ¼ãƒ  {trophy_frame}")
            return trophy_frame
        else:
            self.logger.warning("âš ï¸ ãƒˆãƒ­ãƒ•ã‚£ãƒ¼ãƒãƒ¼ã‚ºæ¤œå‡ºå¤±æ•—")
            return None
    
    def _select_representative_frames(self, pose_results: List[Dict], trophy_frame: Optional[int]) -> List[int]:
        """ä»£è¡¨ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é¸æŠ"""
        total_frames = len(pose_results)
        
        if trophy_frame is not None:
            # ãƒˆãƒ­ãƒ•ã‚£ãƒ¼ãƒãƒ¼ã‚ºå‰å¾Œ30ãƒ•ãƒ¬ãƒ¼ãƒ ã§ç­‰é–“éš”5æš
            window = 30
            start_frame = max(trophy_frame - window, 0)
            end_frame = min(trophy_frame + window, total_frames - 1)
            frame_indices = np.linspace(start_frame, end_frame, num=5, dtype=int).tolist()
        else:
            # ä¿é™ºã¨ã—ã¦å…¨ä½“ã‹ã‚‰å‡ç­‰5æš
            frame_indices = np.linspace(0, total_frames - 1, num=5, dtype=int).tolist()
        
        return frame_indices
    
    def _generate_images(self, video_path: str, pose_results: List[Dict], 
                        frame_indices: List[int], output_dir: str) -> List[Dict]:
        """ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ç”»åƒã‚’ç”Ÿæˆ"""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise IOError(f"å‹•ç”»ã‚’é–‹ã‘ã¾ã›ã‚“: {video_path}")
        
        saved_images = []
        
        try:
            for idx, frame_no in enumerate(frame_indices):
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)
                ret, frame = cap.read()
                if not ret:
                    self.logger.warning(f"âŒ ãƒ•ãƒ¬ãƒ¼ãƒ èª­ã¿è¾¼ã¿å¤±æ•—: ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå· {frame_no}")
                    continue
                
                # ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤æç”»
                pose_data = pose_results[frame_no] if frame_no < len(pose_results) else {}
                annotated_frame = self._draw_pose_landmarks(frame, pose_data)
                
                # ç”»åƒä¿å­˜
                filename = f"pose_{idx:03d}.jpg"
                save_path = os.path.join(output_dir, filename)
                success = cv2.imwrite(save_path, annotated_frame)
                
                if success:
                    self.logger.info(f"âœ… ä¿å­˜æˆåŠŸ: {filename}")
                    saved_images.append({
                        'filename': filename,
                        'path': save_path,
                        'frame_number': frame_no,
                        'index': idx
                    })
                else:
                    self.logger.error(f"âŒ ä¿å­˜å¤±æ•—: {filename}")
        
        finally:
            cap.release()
        
        return saved_images
    
    def _draw_pose_landmarks(self, frame: np.ndarray, pose_data: Dict) -> np.ndarray:
        """ãƒãƒ¼ã‚ºãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã‚’æç”»"""
        if not pose_data.get("has_pose") or not pose_data.get("landmarks"):
            return frame
        
        landmarks = pose_data["landmarks"]
        annotated_frame = frame.copy()
        
        # æ¥ç¶šç·šã®å®šç¾©
        connections = [
            # èƒ´ä½“
            ("left_shoulder", "right_shoulder"),
            ("left_shoulder", "left_hip"),
            ("right_shoulder", "right_hip"),
            ("left_hip", "right_hip"),
            
            # å·¦è…•
            ("left_shoulder", "left_elbow"),
            ("left_elbow", "left_wrist"),
            
            # å³è…•
            ("right_shoulder", "right_elbow"),
            ("right_elbow", "right_wrist"),
            
            # å·¦è„š
            ("left_hip", "left_knee"),
            ("left_knee", "left_ankle"),
            
            # å³è„š
            ("right_hip", "right_knee"),
            ("right_knee", "right_ankle"),
        ]
        
        # æ¥ç¶šç·šã‚’æç”»
        for start_point, end_point in connections:
            start_landmark = landmarks.get(start_point)
            end_landmark = landmarks.get(end_point)
            
            if start_landmark and end_landmark:
                start_pos = (int(start_landmark["x"]), int(start_landmark["y"]))
                end_pos = (int(end_landmark["x"]), int(end_landmark["y"]))
                cv2.line(annotated_frame, start_pos, end_pos, (0, 255, 0), 2)
        
        # ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æç”»
        for landmark_name, landmark in landmarks.items():
            if landmark:
                pos = (int(landmark["x"]), int(landmark["y"]))
                cv2.circle(annotated_frame, pos, 5, (0, 0, 255), -1)
        
        return annotated_frame

